{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patch_responses.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with data from PaStA \n",
    "The \"pasta aggregate\" script aggregates/extracts variousinformation from the mbox-result of PaStA.\n",
    "The responses option extracts and dumps the data for mbox-clusters with patches and all associated\n",
    "emails and commits as a pickled dictionary. These can be further\n",
    "used for input to various analyses on code review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some preprocessing and exploration\n",
    "We explore some numbers on the input data, like patch count, commit counts, data types etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.patch_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null/NaN patch_ids\n",
    "response_df.fillna({'patch_id':'_'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df['upstream'] = response_df['upstream'].map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.index.name = \"idx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.set_index(['cluster_id', 'patch_id'], append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denormalize responses\n",
    "The responses column is a dict type with different attributes like mesg_id, parent (parent thread'd mesg id), and the actual message (bytestring) itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_responses = pd.melt(response_df.responses.apply(pd.Series).reset_index(), \n",
    "            id_vars=['idx', 'cluster_id', 'patch_id'],\n",
    "            value_name='responses').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_responses.drop('variable', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library is a wrapper around json_normalize. Due to NaNs in the columns (no responses for some patches). \n",
    "# Ideally one could also use json_normalize, but due to NaNs it would't be straightforward.\n",
    "# In principle we could directly use the flat_table on the list of dicts instead of the melt step above, \n",
    "# but that somehow did not work\n",
    "import flat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_responses = flat_table.normalize(df_melt_responses, expand_dicts=True, expand_lists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_responses.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_responses.to_csv(\"df_with_responses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denormalize upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_upstream = pd.melt(response_df.upstream.apply(pd.Series).reset_index(),\n",
    "             id_vars=['idx', 'cluster_id', 'patch_id'],\n",
    "             value_name='upstream').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_upstream.drop('variable', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_upstream.to_csv(\"df_with_upstream.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1 = dd.read_csv(\"df_with_responses.csv\", blocksize=1e9, dtype={\"cluster_id \": \"int32\", \"patch_id \": \"category\", \\\n",
    "                                                                 \"responses.resp_msg_id\": \"category\", \\\n",
    "                                                                 \"responses.parent\": \"category\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd2 = dd.read_csv(\"df_with_upstream.csv\", blocksize=1e9, dtype={\"cluster_id \": \"int32\", \"patch_id \": \"category\", \\\n",
    "                                                               \"upstream\": \"category\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dask_final = dd.merge(dd1, dd2, left_index=True, right_index=True, how='left') \\\n",
    ".drop(['patch_id_y', 'cluster_id_y', 'idx_y'], axis=1) \\\n",
    ".reset_index(drop=True) \\\n",
    ".rename(columns={\"idx_x\": \"idx\", \"cluster_id_x\": \"cluster_id\", \"patch_id_x\": \"patch_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the dataframe (otherwise the computation is lazy)\n",
    "df_dask_final.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be executed directly, instead of compute above to save the frame as a single file\n",
    "df_dask_final.to_csv(\"df_dask_final.csv\", single_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary if reading the final dataframe from disk. Reading with Dask gives \n",
    "# the advantage of using the resources better (blocksize parameter), dtypes are tuned to reduce memory usage.\n",
    "final = dd.read_csv(\"df_dask_final.csv\", blocksize=50e7, dtype={\"cluster_id \": \"int32\", \"patch_id \": \"category\", \\\n",
    "                                                                 \"responses.resp_msg_id\": \"category\", \\\n",
    "                                                                 \"responses.parent\": \"category\", \\\n",
    "                                                                 \"upstream\": \"category\"}).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "final = final.compute(num_workers=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently, duplicates can only be eliminated after converting to pandas. I suspect, while Dask is merging \n",
    "# several distributed dataframes, all duplicates cannot be detected. They are only found when the results \n",
    "# are collected as a whole\n",
    "final.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size considerable reduced than df_dask_final\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe - intermediate denormalized data for the kernel patches with email response \n",
    "# and commit data\n",
    "final.to_csv(\"df_pd_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pasta",
   "language": "python",
   "name": "venv_pasta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
